# InternVL2 ì‹œë¦¬ì¦ˆ í€µ ìŠ¤íƒ€íŠ¸

`transformers`ë¥¼ ì‚¬ìš©í•˜ì—¬ InternVL2 ì‹œë¦¬ì¦ˆë¥¼ ì‹¤í–‰í•˜ëŠ” ì˜ˆì œ ì½”ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ë˜í•œ [ì˜¨ë¼ì¸ ë°ëª¨](https://internvl.opengvlab.com/)ì—ì„œ InternVL2 ì‹œë¦¬ì¦ˆ ëª¨ë¸ì„ ê²½í—˜í•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.

> ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ë ¤ë©´ transformers==4.37.2ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

## ëª¨ë¸ ì¤€ë¹„

| ëª¨ë¸ ì´ë¦„           | íƒ€ì… | íŒŒë¼ë¯¸í„° | ë‹¤ìš´ë¡œë“œ                                                            |  í¬ê¸°  |
| -------------------- | ---- | ----- | ------------------------------------------------------------------- | :----: |
| InternVL2-1B         | MLLM | 0.9B  | ğŸ¤— [HF ë§í¬](https://huggingface.co/OpenGVLab/InternVL2-1B)         | 1.8 GB |
| InternVL2-2B         | MLLM | 2.2B  | ğŸ¤— [HF ë§í¬](https://huggingface.co/OpenGVLab/InternVL2-2B)         | 4.2 GB |
| InternVL2-4B         | MLLM | 4.2B  | ğŸ¤— [HF ë§í¬](https://huggingface.co/OpenGVLab/InternVL2-4B)         | 7.8 GB |
| InternVL2-8B         | MLLM | 8.1B  | ğŸ¤— [HF ë§í¬](https://huggingface.co/OpenGVLab/InternVL2-8B)         | 16 GB  |
| InternVL2-26B        | MLLM | 25.5B | ğŸ¤— [HF ë§í¬](https://huggingface.co/OpenGVLab/InternVL2-26B)        | 48 GB  |
| InternVL2-40B        | MLLM | 40.1B | ğŸ¤— [HF ë§í¬](https://huggingface.co/OpenGVLab/InternVL2-40B)        | 75 GB  |
| InternVL2-Llama3-76B | MLLM | 76.3B | ğŸ¤— [HF ë§í¬](https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B) | 143 GB |

í•„ìš”ì— ë”°ë¼ ìœ„ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ `pretrained/` í´ë”ì— ë„£ìœ¼ì‹­ì‹œì˜¤.

```sh
cd pretrained/
# pip install -U huggingface_hub
# OpenGVLab/InternVL2-1B ë‹¤ìš´ë¡œë“œ
huggingface-cli download --resume-download --local-dir-use-symlinks False OpenGVLab/InternVL2-1B --local-dir InternVL2-1B
# OpenGVLab/InternVL2-2B ë‹¤ìš´ë¡œë“œ
huggingface-cli download --resume-download --local-dir-use-symlinks False OpenGVLab/InternVL2-2B --local-dir InternVL2-2B
# OpenGVLab/InternVL2-4B ë‹¤ìš´ë¡œë“œ
huggingface-cli download --resume-download --local-dir-use-symlinks False OpenGVLab/InternVL2-4B --local-dir InternVL2-4B
# OpenGVLab/InternVL2-8B ë‹¤ìš´ë¡œë“œ
huggingface-cli download --resume-download --local-dir-use-symlinks False OpenGVLab/InternVL2-8B --local-dir InternVL2-8B
# OpenGVLab/InternVL2-26B ë‹¤ìš´ë¡œë“œ
huggingface-cli download --resume-download --local-dir-use-symlinks False OpenGVLab/InternVL2-26B --local-dir InternVL2-26B
# OpenGVLab/InternVL2-40B ë‹¤ìš´ë¡œë“œ
huggingface-cli download --resume-download --local-dir-use-symlinks False OpenGVLab/InternVL2-40B --local-dir InternVL2-40B
# OpenGVLab/InternVL2-Llama3-76B ë‹¤ìš´ë¡œë“œ
huggingface-cli download --resume-download --local-dir-use-symlinks False OpenGVLab/InternVL2-Llama3-76B --local-dir InternVL2-Llama3-76B
```

ë””ë ‰í† ë¦¬ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```sh
pretrained
â”œâ”€â”€ InternVL2-1B
â”œâ”€â”€ InternVL2-2B
â”œâ”€â”€ InternVL2-4B
â”œâ”€â”€ InternVL2-8B
â”œâ”€â”€ InternVL2-26B
â”œâ”€â”€ InternVL2-40B
â””â”€â”€ InternVL2-Llama3-76B
```

### ëª¨ë¸ ë¡œë”©

#### 16-bit (bf16 / fp16)

`````{tabs}

````{tab} 1B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-1B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval().cuda()
```
````

````{tab} 2B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-2B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval().cuda()
```
````

````{tab} 4B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-4B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval().cuda()
```
````

````{tab} 8B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-8B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval().cuda()
```
````

````{tab} 26B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-26B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval().cuda()
```
````

````{tab} 40B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-40B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval().cuda()
```
````

````{tab} 76B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-Llama3-76B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval().cuda()
```
````

`````

#### BNB 8-bit ì–‘ìí™”

`````{tabs}

````{tab} 1B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-1B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_8bit=True,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval()
```

````

````{tab} 2B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-2B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_8bit=True,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval()
```

````

````{tab} 4B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-4B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_8bit=True,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval()
```

````

````{tab} 8B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-8B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_8bit=True,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval()
```

````

````{tab} 26B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-26B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_8bit=True,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval()
```

````

````{tab} 40B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-40B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_8bit=True,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval()
```

````

````{tab} 76B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-Llama3-76B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_8bit=True,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval()
```

````

`````

#### BNB 4-bit ì–‘ìí™”

`````{tabs}

````{tab} 1B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-1B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_4bit=True,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval()
```

````

````{tab} 2B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-2B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_4bit=True,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval()
```

````

````{tab} 4B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-4B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_4bit=True,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval()
```

````

````{tab} 8B

```python
import torch
from transformers import AutoTokenizer, AutoModel
path = "OpenGVLab/InternVL2-8B"
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    load_in_4bit=True,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True).eval()
```

````

````{tab} 26B

> **âš ï¸ ê²½ê³ :** InternViT-6Bì—ì„œ BNB 4-bit ì–‘ìí™”ì˜ ì‹¬ê°í•œ ì–‘ìí™” ì˜¤ë¥˜ë¡œ ì¸í•´ ëª¨ë¸ì´ ë¹„ì •ìƒì ì¸ ì¶œë ¥ì„ ìƒì„±í•˜ê³  ì´ë¯¸ì§€ë¥¼ ì´í•´í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ BNB 4-bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

````

````{tab} 40B

> **âš ï¸ ê²½ê³ :** InternViT-6Bì—ì„œ BNB 4-bit ì–‘ìí™”ì˜ ì‹¬ê°í•œ ì–‘ìí™” ì˜¤ë¥˜ë¡œ ì¸í•´ ëª¨ë¸ì´ ë¹„ì •ìƒì ì¸ ì¶œë ¥ì„ ìƒì„±í•˜ê³  ì´ë¯¸ì§€ë¥¼ ì´í•´í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ BNB 4-bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

````

````{tab} 76B

> **âš ï¸ ê²½ê³ :** InternViT-6Bì—ì„œ BNB 4-bit ì–‘ìí™”ì˜ ì‹¬ê°í•œ ì–‘ìí™” ì˜¤ë¥˜ë¡œ ì¸í•´ ëª¨ë¸ì´ ë¹„ì •ìƒì ì¸ ì¶œë ¥ì„ ìƒì„±í•˜ê³  ì´ë¯¸ì§€ë¥¼ ì´í•´í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ BNB 4-bit ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

````

`````

#### ë‹¤ì¤‘ GPU

ì½”ë“œë¥¼ ì´ë ‡ê²Œ ì‘ì„±í•˜ëŠ” ì´ìœ ëŠ” ë‹¤ì¤‘ GPU ì¶”ë¡  ì¤‘ì— í…ì„œê°€ ë™ì¼í•œ ì¥ì¹˜ì— ìˆì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì²« ë²ˆì§¸ ë ˆì´ì–´ì™€ ë§ˆì§€ë§‰ ë ˆì´ì–´ê°€ ë™ì¼í•œ ì¥ì¹˜ì— ìˆë„ë¡ ë³´ì¥í•¨ìœ¼ë¡œì¨ ì´ëŸ¬í•œ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.

```python
import math
import torch
from transformers import AutoTokenizer, AutoModel

def split_model(model_name):
    device_map = {}
    world_size = torch.cuda.device_count()
    num_layers = {
        'InternVL2-1B': 24, 'InternVL2-2B': 24, 'InternVL2-4B': 32, 'InternVL2-8B': 32,
        'InternVL2-26B': 48, 'InternVL2-40B': 60, 'InternVL2-Llama3-76B': 80}[model_name]
    # ì²« ë²ˆì§¸ GPUëŠ” ViTì— ì‚¬ìš©ë˜ë¯€ë¡œ ì ˆë°˜ì˜ GPUë¡œ ì·¨ê¸‰í•©ë‹ˆë‹¤.
    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))
    num_layers_per_gpu = [num_layers_per_gpu] * world_size
    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)
    layer_cnt = 0
    for i, num_layer in enumerate(num_layers_per_gpu):
        for j in range(num_layer):
            device_map[f'language_model.model.layers.{layer_cnt}'] = i
            layer_cnt += 1
    device_map['vision_model'] = 0
    device_map['mlp1'] = 0
    device_map['language_model.model.tok_embeddings'] = 0
    device_map['language_model.model.embed_tokens'] = 0
    device_map['language_model.output'] = 0
    device_map['language_model.model.norm'] = 0
    device_map['language_model.lm_head'] = 0
    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0

    return device_map
```

`````{tabs}

````{tab} 1B
```python
path = "OpenGVLab/InternVL2-1B"
device_map = split_model('InternVL2-1B')
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True,
    device_map=device_map).eval()
```
````

````{tab} 2B
```python
path = "OpenGVLab/InternVL2-2B"
device_map = split_model('InternVL2-2B')
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True,
    device_map=device_map).eval()
```
````

````{tab} 4B
```python
path = "OpenGVLab/InternVL2-4B"
device_map = split_model('InternVL2-4B')
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    use_flash_attn=True,
    trust_remote_code=True,
    device_map=device_map).eval()
```
````

````{tab} 8B
```python
path = "OpenGVLab/InternVL2-8B"
device_map = split_model('InternVL2-8B')
model = AutoModel.from_pretrained(
    path,
    torch
